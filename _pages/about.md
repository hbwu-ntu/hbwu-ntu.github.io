---
permalink: /
title: ""
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---


# About Me

Hi! I'm Haibin Wu, a senior applied scientist at Microsoft. I was a Ph.D. degree at National Taiwan University (NTU), working with Prof. [Hung-yi Lee](http://speech.ee.ntu.edu.tw/~tlkagk/) and Prof. [Lin-shan Lee](http://speech.ee.ntu.edu.tw/previous_version/lslNew.htm) in the area of machine learning and speech processing. My expertise lies in speech foundation models, neural audio codecs, prompt engineer, speech LLMs, speech enhancement, and deepfake detection. By the way. I was fortunate enough to be funded by a [Google PhD Fellowship](https://research.google/outreach/phd-fellowship/recipients/). I'm a main contributor for [S3PRL](https://github.com/s3prl/s3prl) v0.4.0 with 2200+ GitHub stars. I have a keen interest in photography, and you can find my portfolio on my [homepage](https://www.mipai.com.cn/frankwu).

<!-- 
[Publications](#publications) / [Teaching](#teaching) / [Honors](#honors) / [Side Projects](#projects) / [CV](files/cv.pdf)
-->
<!-- 
/ [Talks](#Talks) 
-->

# Selected Publications

- **Laugh Now Cry Later: Controlling Time-Varying Emotional States of Flow-Matching-Based Zero-Shot Text-to-Speech**<br/>
    <u>Haibin Wu</u>, Xiaofei Wang, Sefik Emre Eskimez, Manthan Thakker, Daniel Tompkins, Chung-Hsien Tsai, Canrun Li, Zhen Xiao, Sheng Zhao, Jinyu Li, Naoyuki Kanda<br/>
    *SLT 2024*<br/>
    [ [pdf](https://arxiv.org/abs/2407.12229) | [Webpage](https://www.microsoft.com/en-us/research/project/emoctrl-tts/) | [Github](https://github.com/hbwu-ntu/EmoCtrlTTS-Eval)]

- **Ultra-Low Latency Speech Enhancement - A Comprehensive Study**<br/>
    <u>Haibin Wu</u>, Sebastian Braun<br/>
    *Preprint*<br/>
    [ [pdf](https://arxiv.org/abs/2406.07237)]

- **SpeechPrompt: Prompting Speech Language Models for Speech Processing Tasks**<br/>
    Kai-Wei Chang, <u>Haibin Wu</u>, Yu-Kai Wang, Yuan-Kuei Wu, Hua Shen, Wei-Cheng Tseng, Iu-thing Kang, Shang-Wen Li, Hung-yi Lee<br/>
    *TASLP*<br/>
    [ [pdf](https://ieeexplore.ieee.org/abstract/document/10620644) | [Webpage](https://kwchang.org/SpeechPrompt/) | [Github](https://github.com/ga642381/SpeechPrompt)]

- **CodecFake: Enhancing Anti-Spoofing Models Against Deepfake Audios from Codec-Based Speech Synthesis Systems**<br/>
    <u>Haibin Wu</u>, Yuan Tseng, Hung-yi Lee<br/>
    *Interspeech 2024*<br/>
    [ [pdf](https://arxiv.org/abs/2409.10358) | [Webpage](https://github.com/roger-tseng/CodecFake)]

- **Singing Voice Graph Modeling for SingFake Detection**<br/>
    Xuanjun Chen, <u>Haibin Wu</u>, Jyh-Shing Roger Jang, Hung-yi Lee <br/>
    *Interspeech 2024*<br/>
    [ [pdf](https://arxiv.org/abs/2406.03111) | [GitHub](https://github.com/xjchenGit/SingGraph)]

- **EMO-SUPERB: An In-depth Look at Speech Emotion Recognition**<br/>
    <u>Haibin Wu</u>, Huang-Cheng Chou, Kai-Wei Chang, Lucas Goncalves, Jiawei Du, Jyh-Shing Roger Jang, Chi-Chun Lee, Hung-Yi Lee<br/>
    *Preprint*<br/>
    [ [pdf](https://arxiv.org/abs/2402.13018) | [Webpage](https://emosuperb.github.io/) | [Github](https://github.com/EMOsuperb/EMO-SUPERB-submission)]

- **Codec-SUPERB: An In-Depth Analysis of Sound Codec Models**<br/>
    <u>Haibin Wu</u>, Ho-Lam Chung, Yi-Cheng Lin, Yuan-Kuei Wu, Xuanjun Chen, Yu-Chi Pai, Hsiu-Hsuan Wang, Kai-Wei Chang, Alexander H. Liu, Hung-yi Lee<br/>
    *ACL 2024 Finding*<br/>
    [ [pdf](https://arxiv.org/abs/2402.13071) | [Github](https://github.com/voidful/Codec-SUPERB) | [Leaderboard](https://codecsuperb.com/) | [Huggingface](https://huggingface.co/Codec-SUPERB)]

- **Towards audio language modeling - an overview**<br/>
    <u>Haibin Wu</u>, Xuanjun Chen, Yi-Cheng Lin, Kai-wei Chang, Ho-Lam Chung, Alexander H. Liu, Hung-yi Lee<br/>
    *Preprint*<br/>
    [ [pdf](https://arxiv.org/abs/2402.13236)]

- **SpeechGen: Unlocking the Generative Power of Speech Language Models with Prompts**<br/>
    <u>Haibin Wu</u>, Kai-Wei Chang, Yuan-Kuei Wu, Hung-yi Lee<br/>
    *Preprint*<br/>
    [ [pdf](https://arxiv.org/abs/2306.02207) | [Webpage](https://kwchang.org/SpeechPrompt/) | [Github](https://github.com/ga642381/SpeechPrompt)]

- **The defender's perspective on automatic speaker verification: An overview**<br/>
    <u>Haibin Wu</u>, Jiawen Kang, Lingwei Meng, Helen Meng, Hung-yi Lee<br/>
    *IJCAI DADA workshop 2023*<br/>
    [ [pdf](https://arxiv.org/abs/2305.12804)]

- **Rethinking complex-valued deep neural networks for monaural speech enhancement**<br/>
    <u>Haibin Wu</u>, Ke Tan, Buye Xu, Anurag Kumar, Daniel Wong<br/>
    *Interspeech 2023*<br/>
    [ [pdf](https://arxiv.org/abs/2301.04320)]

- **Partially Fake Audio Detection by Self-Attention-Based Fake Span Discovery**<br/>
    <u>Haibin Wu</u>, Heng-Cheng Kuo, Naijun Zheng, Kuo-Hsuan Hung, Hung-Yi Lee, Yu Tsao, Hsin-Min Wang, Helen Meng<br/>
    *ICASSP 2022*<br/>
    [ [pdf](https://arxiv.org/abs/2202.06684) | [video](https://www.youtube.com/watch?v=owPPvwN_Rfc)]

- **Adversarial Sample Detection for Speaker Verification by Neural Vocoders**<br/>
    <u>Haibin Wu</u>, Po-chun Hsu, Ji Gao, Shanshan Zhang, Shen Huang, Jian Kang, Zhiyong Wu, Helen Meng, Hung-yi Lee<br/>
    *ICASSP 2022*<br/>
    [ [pdf](https://arxiv.org/abs/2107.00309) | [Github](https://github.com/HaibinWu666/spot-adv-by-vocoder) | [video](https://youtu.be/7jD6iCzSgCM)]

- **Adversarial attacks on spoofing countermeasures of automatic speaker verification**<br/>
    S Liu, <u>H Wu</u>, H Lee, H Meng<br/>
    *ASRU 2019*<br/>
    [ [pdf](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9003763) ]

For the complete list, please visit [google scholar](https://scholar.google.com.tw/citations?user=-bB-WHEAAAAJ&hl=zh-TW).

# Research Experience
- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">Research scientist intern at Microsoft</span> <span style="flex:  0 0 auto"><i>May 2024 - Aug 2024</i></span></p>

- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">Research scientist intern at Microsoft</span> <span style="flex:  0 0 auto"><i>Feb 2024 - May 2024</i></span></p>

- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">Research scientist intern at Meta</span> <span style="flex:  0 0 auto"><i>May 2023 - Sep 2023</i></span></p>

- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">Applied scientist intern at Amazon</span> <span style="flex:  0 0 auto"><i>Sep 2022 - Dec 2022</i></span></p>

- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">Research scientist intern at Meta</span> <span style="flex:  0 0 auto"><i>May 2022 - Aug 2022</i></span></p>

- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">Visiting Student at the Chinese University of Hong Kong</span> <span style="flex:  0 0 auto"><i>May 2021 - April 2022</i></span></p>

- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">Visiting Student at SIGS of Tsinghua University</span> <span style="flex:  0 0 auto"><i>Aug. 2020 - May 2021</i></span></p>

- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">Intern at Tencent</span> <span style="flex:  0 0 auto"><i>Jan. 2021 - May 2021</i></span></p>


<!-- - <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">Research Assistant at National Tsinghua University</span> <span style="flex:  0 0 auto"><i>Sep. 2018 - Mar. 2019</i></span></p> -->

# Challenge
- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto"><a href="http://addchallenge.cn/add2022">2022 ICASSP ADD challenge track 2</a> </span> <span style="flex:  0 0 auto"><i>Rank: 2/33</i></span></p>
- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto"><a href="https://www.alibabacloud.com/m2met-alimeeting">2022 ICASSP M2MeT challenge track 1</a> </span> <span style="flex:  0 0 auto"><i>Rank: 2/14</i></span></p>
<!-- - <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">[2022 Interspeech SASV challenge](https://sasv-challenge.github.io/)</span> <span style="flex:  0 0 auto"><i>Rank: 8/23</i></span></p> -->

# Honers
- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">Google studnet travel grant</span> <span style="flex:  0 0 auto"><i>Google 2024</i></span></p>
- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">ICASSP travel grant</span> <span style="flex:  0 0 auto"><i>ICASSP 2024</i></span></p>
- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">Interspeech travel grant</span> <span style="flex:  0 0 auto"><i>Interspeech 2022</i></span></p>
- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">Appier Scholarship</span> <span style="flex:  0 0 auto"><i>Appier 2022</i></span></p>
- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">Google PHD Fellowship</span> <span style="flex:  0 0 auto"><i>Google 2021</i></span></p>
- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">Advanced Speech Technologies Scholarship</span> <span style="flex:  0 0 auto"><i>NTU EECS 2020</i></span></p>
- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">Academic Achievement Award</span> <span style="flex:  0 0 auto"><i>NCTU EECS 2019</i></span></p>
- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">Academic Achievement Award</span> <span style="flex:  0 0 auto"><i>NCTU EECS 2018</i></span></p>
- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">National Scholarship</span> <span style="flex:  0 0 auto"><i>Chinese Ministry of Education 2014</i></span></p>

<!-- # Teaching

- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">TA of <a href="https://www.youtube.com/watch?v=Ye018rCVvOo&list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J/">Machine Learning</a></span> <span style="flex:  0 0 auto"><i>NTU EECS, Spring 2021</i></span></p> -->


<!-- # Projects

- Open Sourced End-to-end Speech Recognition System [ [code](https://github.com/Alexander-H-Liu/End-to-end-ASR-Pytorch) ![GitHub stars](https://img.shields.io/github/stars/Alexander-H-Liu/End-to-end-ASR-Pytorch?style=social&label=Star&maxAge=2592000) ]
- Mandarin Spoken QA System [ *[demo](http://deeplearning.website:8080/?fbclid=IwAR1G6mdk34Q9vA29KhKyn7AFNblR2iV3c2N21H7AbKXS9EN9VuFgO8vIrBE)* ] -->
