---
permalink: /
title: ""
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---


# About Me

Hi! I'm Haibin Wu, a senior applied scientist at Microsoft. I got my Ph.D. degree at National Taiwan University, working with Prof. [Hung-yi Lee](http://speech.ee.ntu.edu.tw/~tlkagk/) and Prof. [Lin-shan Lee](http://speech.ee.ntu.edu.tw/previous_version/lslNew.htm) in the area of machine learning and speech processing. My expertise lies in speech language models, speech generation, neural audio codecs, speech enhancement, and deepfake detection. By the way, I was fortunate enough to be funded by a [Google PhD Fellowship](https://research.google/outreach/phd-fellowship/recipients/). I'm a main contributor for [S3PRL](https://github.com/s3prl/s3prl) with 2400+ GitHub stars. I have a keen interest in photography, and you can find my portfolio on my [homepage](https://www.mipai.com.cn/frankwu).

<!-- 
[Publications](#publications) / [Teaching](#teaching) / [Honors](#honors) / [Side Projects](#projects) / [CV](files/cv.pdf)
-->
<!-- 
/ [Talks](#Talks) 
-->

# Selected Publications

- **Towards Efficient Speech-Text Jointly Decoding within One Speech Language Model**<br/>
    <u>Haibin Wu†</u>, Yuxuan Hu†, Ruchao Fan, Xiaofei Wang, Kenichi Kumatani, Bo Ren, Jianwei Yu, Heng Lu, Lijuan Wang, Yao Qian, Jinyu Li<br/>
    [ [pdf](https://arxiv.org/pdf/2506.04518)]

- **Phi-Omni-ST: A multimodal language model for direct speech-to-speech translation**<br/>
    <u>Haibin Wu†</u>, Yuxuan Hu†, Ruchao Fan, Xiaofei Wang, Heng Lu, Yao Qian, Jinyu Li<br/>
    [ [pdf](https://arxiv.org/pdf/2506.04392)]

- **On The Landscape of Spoken Language Models: A Comprehensive Survey**<br/>
    <u>Haibin Wu†</u>, Siddhant Arora†, Kai-Wei Chang†, Chung-Ming Chien†, Yifan Peng†, Yossi Adi, Emmanuel Dupoux, Hung-Yi Lee, Karen Livescu, Shinji Watanabe<br/>
    [ [pdf](https://arxiv.org/pdf/2504.08528)]

- **TS3-Codec: Transformer-Based Simple Streaming Single Codec**<br/>
    <u>Haibin Wu</u>, Naoyuki Kanda, Sefik Emre Eskimez, Jinyu Li<br/>
    *Interspeech 2025*<br/>
    [ [pdf](https://arxiv.org/abs/2411.18803)]

- **Laugh Now Cry Later: Controlling Time-Varying Emotional States of Flow-Matching-Based Zero-Shot Text-to-Speech**<br/>
    <u>Haibin Wu</u>, Xiaofei Wang, Sefik Emre Eskimez, Manthan Thakker, Daniel Tompkins, Chung-Hsien Tsai, Canrun Li, Zhen Xiao, Sheng Zhao, Jinyu Li, Naoyuki Kanda<br/>
    *SLT 2024*<br/>
    [ [pdf](https://arxiv.org/abs/2407.12229) | [Webpage](https://www.microsoft.com/en-us/research/project/emoctrl-tts/) | [Github](https://github.com/hbwu-ntu/EmoCtrlTTS-Eval)]

- **Ultra-Low Latency Speech Enhancement - A Comprehensive Study**<br/>
    <u>Haibin Wu</u>, Sebastian Braun<br/>
    *ICASSP 2025*<br/>
    [ [pdf](https://arxiv.org/abs/2406.07237)]

- **Codec-SUPERB: An In-Depth Analysis of Sound Codec Models**<br/>
    <u>Haibin Wu</u>, Ho-Lam Chung, Yi-Cheng Lin, Yuan-Kuei Wu, Xuanjun Chen, Yu-Chi Pai, Hsiu-Hsuan Wang, Kai-Wei Chang, Alexander H. Liu, Hung-yi Lee<br/>
    *ACL 2024 Finding*<br/>
    [ [pdf](https://arxiv.org/abs/2402.13071) | [Github](https://github.com/voidful/Codec-SUPERB) | [Leaderboard](https://codecsuperb.com/) | [Huggingface](https://huggingface.co/Codec-SUPERB)]

- **CodecFake: Enhancing Anti-Spoofing Models Against Deepfake Audios from Codec-Based Speech Synthesis Systems**<br/>
    <u>Haibin Wu</u>, Yuan Tseng, Hung-yi Lee<br/>
    *Interspeech 2024*<br/>
    [ [pdf](https://arxiv.org/abs/2409.10358) | [Webpage](https://github.com/roger-tseng/CodecFake)]

- **SpeechGen: Unlocking the Generative Power of Speech Language Models with Prompts**<br/>
    <u>Haibin Wu</u>, Kai-Wei Chang, Yuan-Kuei Wu, Hung-yi Lee<br/>
    [ [pdf](https://arxiv.org/abs/2306.02207) | [Webpage](https://kwchang.org/SpeechPrompt/) | [Github](https://github.com/ga642381/SpeechPrompt)]

- **Rethinking complex-valued deep neural networks for monaural speech enhancement**<br/>
    <u>Haibin Wu</u>, Ke Tan, Buye Xu, Anurag Kumar, Daniel Wong<br/>
    *Interspeech 2023*<br/>
    [ [pdf](https://arxiv.org/abs/2301.04320)]

For the complete list, please visit [google scholar](https://scholar.google.com.tw/citations?user=-bB-WHEAAAAJ&hl=zh-TW).

# Research Experience
- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">Senior applied scientist at Microsoft</span> <span style="flex:  0 0 auto"><i>Sep 2024 - Now</i></span></p>

- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">Research scientist intern at Microsoft</span> <span style="flex:  0 0 auto"><i>May 2024 - Aug 2024</i></span></p>

- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">Research scientist intern at Microsoft</span> <span style="flex:  0 0 auto"><i>Feb 2024 - May 2024</i></span></p>

- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">Research scientist intern at Meta</span> <span style="flex:  0 0 auto"><i>May 2023 - Sep 2023</i></span></p>

- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">Applied scientist intern at Amazon</span> <span style="flex:  0 0 auto"><i>Sep 2022 - Dec 2022</i></span></p>

- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">Research scientist intern at Meta</span> <span style="flex:  0 0 auto"><i>May 2022 - Aug 2022</i></span></p>

- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">Visiting Student at the Chinese University of Hong Kong</span> <span style="flex:  0 0 auto"><i>May 2021 - April 2022</i></span></p>

- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">Visiting Student at SIGS of Tsinghua University</span> <span style="flex:  0 0 auto"><i>Aug. 2020 - May 2021</i></span></p>

- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">Intern at Tencent</span> <span style="flex:  0 0 auto"><i>Jan. 2021 - May 2021</i></span></p>


<!-- - <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">Research Assistant at National Tsinghua University</span> <span style="flex:  0 0 auto"><i>Sep. 2018 - Mar. 2019</i></span></p> -->

# Challenge
- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto"><a href="http://addchallenge.cn/add2022">2022 ICASSP ADD challenge track 2</a> </span> <span style="flex:  0 0 auto"><i>Rank: 2/33</i></span></p>
- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto"><a href="https://www.alibabacloud.com/m2met-alimeeting">2022 ICASSP M2MeT challenge track 1</a> </span> <span style="flex:  0 0 auto"><i>Rank: 2/14</i></span></p>
<!-- - <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">[2022 Interspeech SASV challenge](https://sasv-challenge.github.io/)</span> <span style="flex:  0 0 auto"><i>Rank: 8/23</i></span></p> -->

# Honers
- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">Google studnet travel grant</span> <span style="flex:  0 0 auto"><i>Google 2024</i></span></p>
- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">ICASSP travel grant</span> <span style="flex:  0 0 auto"><i>ICASSP 2024</i></span></p>
- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">Interspeech travel grant</span> <span style="flex:  0 0 auto"><i>Interspeech 2022</i></span></p>
- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">Appier Scholarship</span> <span style="flex:  0 0 auto"><i>Appier 2022</i></span></p>
- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">Google PHD Fellowship</span> <span style="flex:  0 0 auto"><i>Google 2021</i></span></p>
- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">Advanced Speech Technologies Scholarship</span> <span style="flex:  0 0 auto"><i>NTU EECS 2020</i></span></p>
- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">Academic Achievement Award</span> <span style="flex:  0 0 auto"><i>NCTU EECS 2019</i></span></p>
- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">Academic Achievement Award</span> <span style="flex:  0 0 auto"><i>NCTU EECS 2018</i></span></p>
- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">National Scholarship</span> <span style="flex:  0 0 auto"><i>Chinese Ministry of Education 2014</i></span></p>

<!-- # Teaching

- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">TA of <a href="https://www.youtube.com/watch?v=Ye018rCVvOo&list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J/">Machine Learning</a></span> <span style="flex:  0 0 auto"><i>NTU EECS, Spring 2021</i></span></p> -->


<!-- # Projects

- Open Sourced End-to-end Speech Recognition System [ [code](https://github.com/Alexander-H-Liu/End-to-end-ASR-Pytorch) ![GitHub stars](https://img.shields.io/github/stars/Alexander-H-Liu/End-to-end-ASR-Pytorch?style=social&label=Star&maxAge=2592000) ]
- Mandarin Spoken QA System [ *[demo](http://deeplearning.website:8080/?fbclid=IwAR1G6mdk34Q9vA29KhKyn7AFNblR2iV3c2N21H7AbKXS9EN9VuFgO8vIrBE)* ] -->
